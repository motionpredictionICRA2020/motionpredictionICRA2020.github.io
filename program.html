<!DOCTYPE HTML>
<!--
	Phantom by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Long-term Human Motion Prediction</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">
		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
				<header id="header">
					<div class="inner">

							<a href="index.html" class="logo">
								<span class="symbol"><img src="images/lhmp-icon.svg" alt="" /></span><span class="title">LHMP2020</span>
							</a>
						<!-- Nav -->
							<nav>
								<ul>
									<li><a href="#menu">Menu</a></li>
								</ul>
							</nav>

					</div>
				</header>

				<!-- Menu -->
                <nav id="menu">
                    <h2>Menu</h2>
                    <ul>
                        <li><a href="index.html">Home</a></li>
                        <li><a href="program.html">Program</a></li>
                        <li><a href="callforpapers.html">Call for Papers</a></li>
                        <li><a href="challenge.html">Challenge</a></li>
                        <li><a href="pastevents.html">Past Events</a></li>
                    </ul>
                </nav>


				<!-- Main -->
					<div id="main">
						<div class="inner">
							<h1>Program</h1>
							<!-- Program -->
							<section>
								<h2>Program</h2>
								<p>Program of this half-day workshop includes 7 invited talks, a poster session (see below) and the trajectory prediction challenge. The main workshop track will be held in this Zoom room: <a href="https://epfl.zoom.us/j/95756947672">https://epfl.zoom.us/j/95756947672</a>. Additional rooms will be open during the poster session, see the table below.</p>
								<div class="table-wrapper">
									<table>
										<thead>
											<tr>
												<th>Time CEST (PST)</th>
												<th>Speaker</th>
												<th>Talk</th>
											</tr>
										</thead>
										<tbody>
											<tr>
												<td>16:30 - 16:40 (7:30 - 7:40)</td>
												<td>Organizers</td>
												<td>Welcome and Introduction</td>
											</tr>
											<tr>
												<th>16:40 - 17:00 (7:40 - 8:00)</th>
												<th><a href="https://faculty.skoltech.ru/people/gonzaloferrer">Gonzalo Ferrer, Skoltech</a>
												   </th>
												<th>Human Motion Prediction for Social Robot Navigation</th>
											</tr>
											<tr>
												<td colspan="3">This talk will be focused on human motion prediction from the point of view of social robot navigation in urban environments. We will show how predicting the state of dynamic agents, that is, pedestrians, has a profound impact on robot navigation and the other way around, how robot actions affect motion prediction. We will also discuss on the importance of computationally fast solutions, when applied to real-world navigation and the correct management of risk due to the inherent limitations in prediction. Finally, we would mention the current challenges and some next promising directions in motion prediction.</td>
											</tr>
											<tr>
												<th>17:00 - 17:20 (8:00 - 8:20)</th>
												<th><a href="https://web.stanford.edu/~pavone/">Marco Pavone, Stanford</a>
												   </th>
												<th>Multimodal Deep Generative Models for Intent Prediction</th>
											</tr>
											<tr>
												<td colspan="3">In this talk I will present a data-driven approach for learning multimodal interaction dynamics between robot-driven and human-driven vehicles based on recent advances in deep generative modeling. I will then discuss how to incorporate such a learned interaction model into a real-time, interaction-aware decision-making framework.</td>
											</tr>
											<tr>
												<th>17:20 - 17:40 (8:20 - 8:40)</th>
												<th><a href="https://www.linkedin.com/in/dariu-gavrila">Dariu Gavrila, TU Delft</a>
												   </th>
											 <th>Predictive Motion Models for Vulnerable Road Users</th>
											</tr>																
											<tr>
												<td colspan="3">Sensors are meanwhile very good at measuring 3D in the context of environment perception for self-driving vehicles. Scene labeling and object detection have also made big strides, mainly due to advances in deep learning. Time has now come to focus on the next frontier: modeling and anticipating the motion of road users. The potential benefits are large, such as earlier and more effective system reactions in dangerous traffic situations. To reap these benefits, however, it is necessary to use sophisticated predictive motion models based on intent-relevant (context) cues. <br>

												In this talk, I give an overview of predictive motion models and intent-relevant cues with respect to the vulnerable road users (i.e. pedestrians, cyclists). In particular, I discuss the pros and cons of having these models handcrafted by an expert compared to learning them from data. I present results from a recent case study on cyclist path prediction involving a Dynamic Bayesian Network and a Recurrent Neural Network.</td>
											</tr>
											<tr>
												<td>17:40 - 17:50 (8:40 - 8:50)</td>
												<td>Coffee break and setting up posters</td>
												<td></td>
											</tr>
											<tr>
												<td>17:50 - 18:20 (8:50 - 9:20)</td>
												<td>Virtual poster session</td>
												<td></td>
											</tr>
											<tr>
												<th>18:20 - 18:40 (9:20 - 9:40)</th>
												<th><a href="https://www.linkedin.com/in/adrien-gaidon-63ab2358/">Adrien Gaidon, Toyota Research Institute</a>
												   </th>
												<th>Spatiotemporal Relationship Reasoning for Pedestrian Intent Prediction</th>
											</tr>
											<tr>
												<td colspan="3">Forecasting of the next events or actions in videos is a desirable capability for robotics and vision-based applications. In recent years, various models have been developed based on convolution operations for prediction or forecasting, but they lack the ability to reason over spatiotemporal data and infer the relationships of different objects in the scene. In this talk, we will present a framework based on graph convolution to uncover the spatiotemporal relationships in the scene for reasoning about pedestrian intent.  We approach the problem of intent prediction from two different perspectives and anticipate the intention-to-cross within both pedestrian-centric and location-centric scenarios. In addition, we introduce a new dataset designed specifically for autonomous-driving scenarios in areas with dense pedestrian populations: the Stanford-TRI Intent Prediction (STIP) dataset. Our experiments on STIP and the standard JAAD benchmark show that our graph modeling framework is able to outperform the state of the art, predicting the intention-to-cross of the pedestrians with an accuracy of 79.10% on STIP and 79.28% on JAAD, up to one second earlier than when the actual crossing happens.</td>
											</tr>
											<tr>
												<th>18:40 - 19:00 (9:40 - 10:00)</th>
												<th><a href="https://dorsa.fyi/">Dorsa Sadigh, Stanford</a>
												   </th>
												<th>When our Human Modeling Assumptions Fail: The effects of risk, conventions, and non-stationarity on long-term human-robot interaction</th>
											</tr>
											<tr>
												<td colspan="3">Predicting computational models of humans is an important challenge in many robotics applications including autonomous driving, assistive teleoperation, robotic surgery, or interacting in a home with a service robot over a long-period of time. Most current techniques, model-based or data-driven, make strong assumptions about human behaviors. Specifically most techniques assume humans are noisily rational, capable of belief modeling to some extent, or do not adapt significantly over time. In this talk, we discuss settings where these assumptions fail to hold, and provide techniques and overarching paradigms that can capture human behavior even in complex scenarios. We will first discuss how to model human behaviors in near the end of the risk spectrum scenarios. We will then introduce the idea of conventions, i.e., low-dimensional shared representations that capture the interaction and can change over time. Finally, we discuss settings where humans do not simply follow a stationary model, and present reward learning approaches that can discover these non-stationaries.</td>
											</tr>
											<tr>
												<td>19:00 - 19:10 (10:00 - 10:10)</td>
												<td>Coffee break</td>
												<td></td>
											</tr>
											<tr>
												<th>19:10 - 19:30 (10:10 - 10:30)</th>
												<th><a href="https://people.eecs.berkeley.edu/~anca/">Anca Dragan, Berkley</a>
												   </th>
												<th>Data-driven but Safe Prediction</th>
											</tr>
											<tr>
												<td colspan="3">Protecting against the worst-case human motion is too conservative, while fitting a black box policy to human data is too brittle when tested out-of-distribution. In this talk, I'll share our perspective that 1) we have to take into account that human motion is intent-driven when doing prediction, but 2) we also have to find ways to flexibly incorporate this into our predictors to avoid underfitting. I'll go over some examples of achieving this "flexibility" from our recent work. </td>
											</tr>
											<tr>
												<th>19:30 - 19:50 (10:30 - 10:50)</th>
												<th><a href="http://henibenamor.weebly.com/">Heni Ben Amor, Arizona State University</a>
												   </th>
												<th>Learning to Predict and Respond for Human-Robot Interaction</th>
											</tr>
											<tr>
												<td colspan="3">Motion prediction plays a critical role in tasks that involve physical interaction between multiple people. In such scenarios, human interaction partners need to constantly make predictions about each other in order to align their actions in time and space. In this talk, I argue that a tight coupling between motion prediction and response generation is vital for fluent and effective human-robot interaction (HRI). Further, I will present Bayesian Interaction Primitives -- a probabilistic framework that enables learning and inference in such coupled representations for HRI scenarios. Bayesian Interaction Primitives encode the mutual dependencies between interaction partners and can be used to 1.) predict human motion and sensor values, 2.) infer task-relevant latent variables, and 3.) generate appropriate robot repsonses. A critical aspect of this approach is the ability to jointly reason about time and space. I will conclude the talk with a number of application scenarios such as human-robot hugging, collaborative lifting, throwing and catching, or assisted walking with an intelligent prosthesis.</td>
											</tr>
											<tr>
												<td>19:50 - 20:20 (10:50 - 11:20)</td>
												<td><a href="challenge.html">TrajNet++ prediction challenge</a></td>
												<td></td>
											</tr>	
											<tr>
												<td>20:20 - 20:30 (11:20 - 11:30)</td>
												<td>Organizers</td>
												<td>Closing remarks</td>
											</tr>																

										</tbody>
									</table>
								</div>

								<h2>Virtual poster session</h2>
								<p>Each accepted paper will be presented as a virtual poster in a dedicated room during the 30 minute session.</p>
								<div class="table-wrapper">

									<table>
										<thead>
											<tr>
												<th>Authors</th>
												<th>Paper</th>
												<th>Room</th>
											</tr>
										</thead>
										<tbody>
											<tr>
												<td>Ronny Hug, Stefan Becker, Wolfgang Hübner and Michael Arens</td>
												<td>A Short Note on Analyzing Sequence Complexity in Trajectory Prediction Benchmarks <a href="posters/lhmp2020_hug_paper.pdf">[paper]</a> <a href="posters/lhmp2020_hug_poster.pdf">[poster]</a></td>
												<td><a href="https://meet.google.com/yuk-zqif-bqb">https://meet.google.com/yuk-zqif-bqb</a></td>
											</tr>
											<tr>
												<td>Philipp Kratzer, Niteesh Balachandra Midlagajni and Jim Mainprice</td>
												<td>Anticipating Human Intention for Full-Body Motion Prediction <a href="posters/lhmp2020_kratzer_paper.pdf">[paper]</a> <a href="posters/lhmp2020_kratzer_poster.pdf">[poster]</a></td>
												<td><a href="tbd">tbd</a></td>
											</tr>
											<tr>
												<td>Aleksey Postnikov, Alseksander Gamayunov and Gonzalo Ferrer</td>
												<td>HSFM-sigmaNN: Combining a Feedforward Motion Prediction Network and Covariance Prediction <a href="posters/lhmp2020_postnikov_paper.pdf">[paper]</a> <a href="posters/lhmp2020_postnikov_poster.pdf">[poster]</a></td>
												<td><a href="tbd">tbd</a></td>
											</tr>
											<tr>
												<td>Irmak Guzey, Ahmet Ercan Tekden, Evren Samur and Emre Ugur</td>
												<td>Human Motion Prediction With Graph Neural Networks <a href="posters/lhmp2020_guzey_paper.pdf">[paper]</a> <a href="posters/lhmp2020_guzey_poster.pdf">[poster]</a></td>
												<td><a href="https://us04web.zoom.us/j/2429636026?pwd=L1g0TCtxQlZZVlFTbnErV2U4cVhvQT09">https://us04web.zoom.us/j/2429636026?pwd=L1g0TCtxQlZZVlFTbnErV2U4cVhvQT09</a></td>
											</tr>
											<tr>
												<td>Mehmet Hakan Kurtoglu, Yunus Seker, Evren Samur and Emre Ugur</td>
												<td>Predicting Whole Body Motion Trajectories using Conditional Neural Movement Primitives <a href="posters/lhmp2020_kurtoglu_paper.pdf">[paper]</a> <a href="posters/lhmp2020_kurtoglu_poster.pdf">[poster]</a></td>
												<td><a href="https://boun-edu-tr.zoom.us/j/92475953387">https://boun-edu-tr.zoom.us/j/92475953387</a></td>
											</tr>
											<tr>
												<td>Francesco A. N. Palmieri, Krishna R. Pattipati, Giovanni Fioretti, Francesco Verolla, Giovanni Di Gennaro and Amedeo Buonanno</td>
												<td>Exploration/Exploitation in Path Planning Using Probability Propagation <a href="posters/lhmp2020_palmieri_paper.pdf">[paper]</a> <a href="posters/lhmp2020_palmieri_poster.pdf">[poster]</a></td>
												<td><a href="https://us04web.zoom.us/j/77155731274?pwd=d1lwZFExRm56dDlIYUs1endqamt6UT09">https://us04web.zoom.us/j/77155731274?pwd=d1lwZFExRm56dDlIYUs1endqamt6UT09</a></td>
											</tr>
											<tr>
												<td>Eike Rehder</td>
												<td>Prediction: Where To Go Next <a href="posters/lhmp2020_rehder_poster.pdf">[poster]</a> </td>
												<td><a href="https://us04web.zoom.us/j/79671965124?pwd=TG5EbDU4OGVtajhBQ3V2SGpGYUl4dz09">https://us04web.zoom.us/j/79671965124?pwd=TG5EbDU4OGVtajhBQ3V2SGpGYUl4dz09</a></td>
											</tr>														
										</tbody>
									</table>

								</div>

						</div>
					</div>

                <!-- Footer -->
                <footer id="footer">
                    <div class="inner">
                        
                        <section>
                            <h2>Get in touch</h2>
                            <p>Please feel free to send us an <a href="mailto:Andrey.Rudenko@de.bosch.com?Subject=Workshop%20LHMP" target="_top">e-mail</a>
                                , if you have any questions regarding this workshop.</p>
                            <!--
                            <form method="GET" action="mailto:Andrey.Rudenko@de.bosch.com" enctype="text/plain">
                                <div>Subject</div>
                                <input type="text" name="subject" value="[LHMP2020]"/> 
                            
                                <div>Message</div>
                                <textarea name="body"></textarea>
                            
                                <br/>
                                <input type="submit" value="Send" />
                                <input type="reset" name="reset" value="Clear Form" />
                            </form>
                            -->
                        </section>
                        <!-- 
                        <section>
                            <h2>Follow</h2>
                            <ul class="icons">
                                <li><a href="images/Organizers/" class="icon brands style2 fa-twitter"><span class="label">Twitter</span></a></li>
                                <li><a href="#" class="icon brands style2 fa-facebook-f"><span class="label">Facebook</span></a></li>
                                <li><a href="#" class="icon brands style2 fa-instagram"><span class="label">Instagram</span></a></li>
                                <li><a href="#" class="icon brands style2 fa-dribbble"><span class="label">Dribbble</span></a></li>
                                <li><a href="#" class="icon brands style2 fa-github"><span class="label">GitHub</span></a></li>
                                <li><a href="#" class="icon brands style2 fa-500px"><span class="label">500px</span></a></li>
                                <li><a href="#" class="icon solid style2 fa-phone"><span class="label">Phone</span></a></li>
                                <li><a href="#" class="icon solid style2 fa-envelope"><span class="label">Email</span></a></li>
                            </ul>
                        </section>
                        -->
                        <ul class="copyright">
                            <li>&copy; Andrey Rudenko and Luigi Palmieri. All rights reserved</li>
                        </ul>
                    </div>
                </footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>